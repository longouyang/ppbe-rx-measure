---
title: "analyze"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries}
library(MASS)
library(plyr)
library(tidyverse)
library(lubridate)
library(memoise)
```


```{r utilities}
generic.ci_ <- function(x, n = 5000, seed = 1) {
  set.seed(seed)
  lenx = length(x)
  
  structure(
    quantile(
      replicate(n, mean(x[sample.int(lenx, replace = TRUE)])),
      c(0.025, 0.975)),
    names=c("ci.l","ci.u"))
}

generic.ci <- memoise(generic.ci_)
```

# read in data

```{r}
results.dir = "production-results/"
assignments = read_csv(paste0(results.dir, "assignments.csv")) %>%
  mutate(accept.time = ymd_hms(accept.time),
         submit.time = ymd_hms(submit.time),
         duration = difftime(submit.time, accept.time, units = "mins"))
responses = read_csv(paste0(results.dir, "responses.csv"),
                     col_types = cols(string = col_character()))
```


```{r compute-example-correctness}
regexes = c('3a' = 'aaa+',
            'zip-code' = '[0123456789]{5}',
            'consonants-only' = '[bcdfghjklmnpqrstvwxyzBCDFGHJKLMNPQRSTVWXYZ]*',
            'delimiters' = "\\[.*\\]")

example.matches = function(example, rx) {
  res = regexpr(pattern = rx, text = example)
  # make sure we match and that the *entire* string is what matches, not a substring
  res > 0 & attr(res, "match.length") == nchar(example)
}
# example.correct(example = 'aaa', rx = 'aaa+')
# example.correct(example = 'baaa', rx = 'aaa+')
# example.correct(example = 'aaaa', rx = 'aaa+')

responses = responses %>%
  mutate(rx = regexes[rule.id])

responses_match = apply(responses[,c('string','rx')],
      1,
      function(e) { example.matches(example = e['string'], rx = e['rx']) })

responses = mutate(responses,
                   match = responses_match,
                   correct = !xor(polarity == 'positive', match)) %>%
  select(-rx, -rule.desc) # hide these cause they're verbose

# # testing
# View(responses %>% select(rule.id, polarity, string, correct, match) %>% arrange(rule.id, polarity))
```


# user experience

## how long does the task take?

```{r}
qplot(data = assignments,
      x = as.numeric(duration),
      binwidth = 1,
      color = I('white')) + 
  xlab("duration (minutes)")
```


## what did people think was a fair payment?

```{r}
fair.pay = assignments$fair_pay %>% as.numeric %>% na.omit
fair.pay = fair.pay[fair.pay < 5]
qplot(x = fair.pay,
      binwidth = 0.1,
      color = I('white')
      )
```

# research

## how many examples do people give?

```{r, fig.width = 11, fig.height = 3}
e.agg = responses %>% group_by(worker.id, rule.id) %>%
  summarise(num.examples = n()) %>%
  group_by(rule.id, num.examples) %>%
  summarise(freq = n())

xmin = 1 #min(e.agg$num.examples)
xmax = max(e.agg$num.examples)

e.agg$num.examples.fct = factor(e.agg$num.examples, levels = as.character(xmin:xmax))

ggplot(data = e.agg) +
  facet_grid(. ~ rule.id) +
  geom_bar(mapping = aes(x = num.examples.fct, y = freq), stat = 'identity') +
  scale_x_discrete(breaks = as.character(xmin:xmax), drop = FALSE, name = 'number of examples')
```

exactly 2 seems to be common

maybe people are giving exactly 2 because they think that's what i'm asking for?
if this were true, i'd expect there to be people that give all (or mostly) 2-examples and other people that give various ones.

```{r}
e.agg = responses %>% group_by(worker.id, rule.id) %>%
  summarise(num.examples = n()) %>%
  spread(rule.id, num.examples)

# todo: use select()
parcoord(e.agg[,2:5], col = adjustcolor("black", alpha = 0.2))
```

(this doesn't show up in the notebook but it works in the console). still, this graph sucks though. i wish this were easier.

```{r}
e.agg = responses %>% group_by(worker.id, rule.id) %>%
  summarise(num.examples = n()) %>%
  spread(rule.id, num.examples)

names(e.agg) = c("worker.id", paste0("r",gsub("-",".",names(e.agg)[2:5])))
e.agg %>% group_by(r3a, rconsonants.only, rdelimiters, rzip.code) %>%
  summarise(freq=n()) %>% arrange(desc(freq))
```

so it looks like not everyone is giving all 2-examples (though 3/20 people did).
but it looks possible that 7 people thought that you could give at most one example of each polarity (the top 4 rows of this table)

so i think i should still add verbiage saying that (1) you can give as many examples as you want and (2) you should be helpful.

## q: do 2-examples tend to be *balanced*? i.e., one positive and one negative?

```{r}
e.agg = responses %>%
  group_by(worker.id, rule.id) %>%
  mutate(num.examples = length(string)) %>%
  filter(num.examples == 2) %>%
  group_by(worker.id, rule.id) %>%
  summarise(num.pos = sum(polarity == 'positive'),
            num.neg = sum(polarity == 'negative'))

print(table(e.agg$num.pos, e.agg$num.neg))

# proportion test on the frequency of balanced 2-examples
prop.test(x = sum(e.agg$num.pos == 1), nrow(e.agg))
```

overwhelmingly, yes (nb: would need to collect more data for the proportion test, which i believe operates better if the minimum cell count is 5)

### for balanced 2-examples, what is the mean edit distance?

```{r}
e.agg = responses %>%
  group_by(worker.id, rule.id) %>%
  mutate(num.examples = length(string)) %>%
  filter(num.examples == 2, sum(polarity == 'positive') == 1) %>%
  summarise(edit.distance = adist(string)[1,2])

mean.balanced.pair.edit.distance = mean(e.agg$edit.distance)

qplot(data = e.agg,
      x = edit.distance,
      color = I('white'),
      binwidth = 1) + scale_x_continuous(breaks = with(e.agg, min(edit.distance):max(edit.distance))) +
  geom_vline(xintercept = mean.balanced.pair.edit.distance, color = I('red'))
```

are these examples are closer than you'd expect by chance? (permutation test)

```{r}
e.agg = responses %>%
  group_by(worker.id, rule.id) %>%
  mutate(num.examples = length(string)) %>%
  filter(num.examples == 2, sum(polarity == 'positive') == 1)

workers.and.rules = e.agg[,c('worker.id', 'rule.id')]

balanced.example.pair.edit.distance.bootstrap = function(pool) {
  # sample a permuted dataset:
  # for each rule, shuffle all the examples and then pair them off
  # then for each pair, compute edit distance
  
  syn.df = ddply(pool,
        .(rule.id), function(e) {
          pool.pos = e[e$polarity == 'positive',]$string
          pool.neg = e[e$polarity == 'negative',]$string
          
          syn.pos = sample(pool.pos)
          syn.neg = sample(pool.neg)
          
          syn = rbind(syn.pos, syn.neg)
          
          data.frame(distance = apply(X = syn, MARGIN = 2, F = function(pair) { adist(pair)[1,2] }))
        })
  
  mean(syn.df$dist)
}

time = system.time(bootstrap.samples <- replicate(5000, balanced.example.pair.edit.distance.bootstrap(e.agg)))

writeLines(paste0("elapsed seconds: ", round(unname(time['elapsed']), 1)))

writeLines(paste0('95% ci for bootstrap: ', paste0(quantile(bootstrap.samples, c(0.025, 0.0975)), collapse = " - ")))

# one-tailed test: how many bootstrap samples have a mean
# number of clusters less than the observed sample?
sum(bootstrap.samples < mean.balanced.pair.edit.distance) / length(bootstrap.samples)
```


#### the second most likely edit distance for balanced 2-examples is 5: is there a pattern there? or is that just the zip code rule?

```{r}
responses %>%
  group_by(worker.id, rule.id) %>%
  mutate(num.examples = length(string)) %>%
  filter(num.examples == 2, sum(polarity == 'positive') == 1) %>%
  summarise(edit.distance = adist(string)[1,2]) %>%
  filter(edit.distance == 5) %>%
  select(-edit.distance) %>%
  merge(responses)
```

mostly the zip code rule

## how many positive examples versus negative examples?

```{r, fig.width = 11, fig.height = 3}
e.agg = responses %>% group_by(worker.id, rule.id) %>%
  summarise(num.pos = sum(polarity == "positive"),
            num.neg = sum(polarity == "negative")) %>%
  ungroup() %>%
  group_by(num.pos, num.neg, rule.id) %>%
  summarise(freq = n())

qplot(data = e.agg,
      facets = . ~ rule.id,
      x = num.pos,
      y = num.neg,
      size = freq) +
  geom_abline() + 
  scale_x_continuous(name = '# positive examples', breaks = c(0, 3, 6), limits = c(0, 6)) +
  scale_y_continuous(name = '# negative examples', breaks = c(0, 3, 6), limits = c(0, 6))
```

things are somewhat balanced -- people tend to give some negative examples.

### q: do people give more positive examples than negative?

simple check -- paired t-test between number of positive and number of negative examples for each trial X user.

```{r}
e.agg = responses %>% group_by(worker.id, rule.id) %>%
  summarise(num.pos = sum(polarity == "positive"),
            num.neg = sum(polarity == "negative"))

with(e.agg, t.test(num.pos, num.neg, paired = TRUE))
```

there don't appear to be more positive examples than negative, though i want a better model.
TODO: hierarchical model with by-rule and by-user random effects.

## how related are the examples in edit distance?

```{r}
cluster.examples = function(strings, distance.threshold = 2) {
  distance.matrix = adist(strings)
  
  # for each string, figure out which other strings it's similar to
  # (i.e., has edit distance less than the threshold)
  similarities = apply(distance.matrix,
        1,# by row
        function(row) {
          which(row <= distance.threshold)
        })
  clusters = list()
  # print(similarities)
  
  # make the clusters
  Map(1:length(strings),
      f = function(i) {
        # j is the index of the previously created cluster that can contain this string
        j = Position(x = clusters,
                     f = function(cluster) { i %in% cluster })
        
        if (is.na(j)) {
          clusters[[length(clusters) + 1]] <<- similarities[[i]]
        } else {
          clusters[[j]] <<- union(clusters[[j]], similarities[[i]])
        }
      })
  
  # return a list of clusters (each cluster is a vector containing strings that are clustered together)
  # Map(clusters, f = function(indices) { strings[indices] })

  ### WIP ###
  # return a data frame with two columns: string and cluster number
  do.call(rbind,
          Map(clusters,
              1:length(clusters),
              f = function(indices, cluster.label) {
                data.frame(string = strings[indices],
                           cluster.label = cluster.label)
              }
          ))
}

# # testing
strings = c("01234", "012a4", "62804", "628041", "y6280", "0123", "280", "a280b")
# #strings = c('aaa','aa','aaab','baaaab','bbaaabb')
# ## strings = c("94301", "40510", "33333", "r2349", "asdfa", "3621", "834920")
cluster.examples(strings)
```

mean number of clusters for an example sequence:

```{r}
responses.clustered = responses %>%
  group_by(worker.id, rule.id) %>%
  mutate(cluster.label = cluster.examples(string)$cluster.label)

responses.clustered %>%
  group_by(worker.id, rule.id) %>%
  summarise(num.clusters = max(cluster.label)) %>%
  group_by(worker.id) %>%
  summarise(mean_num.clusters = mean(num.clusters)) %>%
  select(mean_num.clusters) %>%
  summary
```

TODO: do there tend to be more negative examples within a cluster? (i think people might come up with one example and then demonstrate various ways it can be perturbed to be a non-example)

TODO: do examples within a cluster tend to be nearby in the sequence of examples the user gave?

comparison to permutation test: sample random participants by sampling from pool of all participants' responses (note that this is sampling *without* replacement, as people wouldn't give the same example twice)
- runtime note: takes around 770 seconds for 5k bootstrap samples. it's a little slow because my clustering function is not vectorized

```{r}
sample.bootstrap.subject = function(worker.id, rule.id) {
  # get examples given by all participants for this rule
  ## written in non-dplyr syntax because i think it might be faster?
  pool = responses[responses$rule.id == rule.id,]
  pool.pos = pool[pool$polarity == 'positive',]$string
  pool.neg = pool[pool$polarity == 'negative',]$string
  
  # get this worker's examples
  this = pool[pool$worker.id == worker.id,]
  
  num.pos = sum(this$polarity == 'positive')
  num.neg = sum(this$polarity == 'negative')
  
  syn.pos = sample(x = pool.pos, size = num.pos, replace = FALSE)
  syn.neg = sample(x = pool.neg, size = num.neg, replace = FALSE)
  
  c(num.clusters = max(cluster.examples(c(syn.pos, syn.neg))$cluster.label))
}

workers.and.rules = responses.clustered[,c('worker.id', 'rule.id')]

clusters.bootstrap = function() {
  num.clusters = apply(workers.and.rules,
                       1,
                       function(e) { 
                         sample.bootstrap.subject(e['worker.id'], e['rule.id'])
                       })
  
  mean(num.clusters)
}

time = system.time(bootstrap.samples <- replicate(500, clusters.bootstrap()))

writeLines(paste0("elapsed seconds: ", round(unname(time['elapsed']), 1)))

writeLines(paste0('95% ci for bootstrap: ', paste0(quantile(bootstrap.samples, c(0.025, 0.0975)), collapse = " - ")))

# one-tailed test: how many bootstrap samples have a mean
# number of clusters less than the observed sample?
sum(bootstrap.samples < mean.num.clusters) / length(bootstrap.samples)
```

TODO: the ways i've done both permutation tests are, i think, reasonable, but there are also reasonable alternatives. talk through my choices with mike, then write down rationale.

## how many mistakes do people make? (e.g., positive examples that don't actually match or negative examples that do match)


by stimulus:

```{r}
responses %>%
  group_by(rule.id) %>%
  summarise(error.rate = sum(!correct) / n())
```

inspecting errors:

```{r}
responses %>% filter(!correct) %>% select(rule.id, worker.id, string, polarity, match, correct) %>% arrange(rule.id)
```

issues
- 3a: ambiguity about lower versus upper case. also, it needs to be clearer that the string contains *only* a's.
- zip-code: the zip code framing might be too semantic. people might be thinking "give examples of *real* zip codes"



by person:
```{r}
responses %>%
  group_by(worker.id) %>%
  summarise(error.rate = sum(!correct) / n()) %>% 
  arrange(desc(error.rate))
```

## how long are the examples that people give?

by stimulus:

```{r, fig.width = 11, fig.height = 3}
qplot(data = responses,
      facets = . ~ rule.id,
      x = nchar(string),
      binwidth = 1,
      color = I('white'))
```

by person:

```{r}
qplot(data = responses,
      x = worker.id,
      y = nchar(string), alpha = I(0.5)) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## do people give examples in particular orders? e.g., shorter ones first or positive ones first?

### length

un-zscored:
```{r}
e = responses %>%
  transform(len = nchar(string)) %>%
  group_by(example.num) %>%
  summarise(mean.len = mean(len),
            cl.len = generic.ci(len)['ci.l'],
            cu.len = generic.ci(len)['ci.u']) %>%
  ungroup

qplot(data = e,
      x = example.num,
      y = mean.len,
      ymin = cl.len,
      ymax = cu.len,
      geom = c('pointrange','line'))
```

note: confidence intervals for last two points are actually huge (very few unique data values, e.g., 1)

un-z-scored, broken down by rule:

```{r}
e = responses %>%
  transform(len = nchar(string)) %>%
  group_by(example.num, rule.id) %>%
  summarise(mean.len = mean(len),
            cl.len = generic.ci(len)['ci.l'],
            cu.len = generic.ci(len)['ci.u']) %>%
  ungroup

qplot(data = e,
      facets = . ~ rule.id,
      x = example.num,
      y = mean.len,
      ymin = cl.len,
      ymax = cu.len,
      geom = c('pointrange','line'))
```


z-scoring length per subject per rule:
```{r}
z.score <- function(xs) {
  centered = xs - mean(xs)
  if (length(xs) == 1) {
    centered
  } else {
    # NB: deliberately doesn't catch the case where all xs are the same
    # because i filter for this later on
    centered / sd(xs)
  }
}

e = responses %>%
  transform(len = nchar(string)) %>%
  group_by(worker.id, rule.id) %>%
  mutate(z.len = z.score(len)) %>%
  mutate(z.len = ifelse(is.nan(z.len), 0, z.len)) %>%
  group_by(example.num) %>%
  summarise(mean.z.len = mean(z.len),
            cl.len = generic.ci(z.len)['ci.l'],
            cu.len = generic.ci(z.len)['ci.u']) %>%
  ungroup

qplot(data = e,
      x = example.num,
      y = mean.z.len,
      ymin = cl.len,
      ymax = cu.len,
      geom = c('pointrange','line'))
```

doesn't appear to be any sequencing over all the participants, though there are certainly cases where this happens:

```{r}
responses %>% filter(worker.id == '26e3c2f', rule.id == '3a')
```

i wonder if such simple->complex sequencing is better for receivers (ERRATUM-minor: i called this d02 task receiving but it's actually sending). i could test this by presenting receivers with different permutations of a given sequence.

break out previous plot by rule.id:

```{r, fig.width = 11, fig.height = 3}
z.score <- function(xs) {
  centered = xs - mean(xs)
  if (length(xs) == 1) {
    centered
  } else {
    # NB: deliberately doesn't catch the case where all xs are the same
    # because i filter for this later on
    centered / sd(xs)
  }
}

e = responses %>%
  transform(len = nchar(string)) %>%
  group_by(worker.id, rule.id) %>%
  mutate(z.len = z.score(len)) %>%
  mutate(z.len = ifelse(is.nan(z.len), 0, z.len)) %>%
  group_by(example.num, rule.id) %>%
  summarise(mean.z.len = mean(z.len),
            cl.len = generic.ci(z.len)['ci.l'],
            cu.len = generic.ci(z.len)['ci.u']) %>%
  ungroup

qplot(data = e,
      facets = . ~ rule.id,
      x = example.num,
      y = mean.z.len,
      ymin = cl.len,
      ymax = cu.len,
      geom = c('pointrange','line'))
```


for each rule, plot each person's length curve individually:

```{r, fig.width = 8, fig.height = 4, dev="svg"}
e = responses %>%
  transform(len = nchar(string)) %>%
  group_by(worker.id, rule.id) %>%
  mutate(z.len = z.score(len), num.examples = max(example.num)) %>%
  mutate(z.len = ifelse(is.nan(z.len), 0, z.len)) %>%
  filter(num.examples > 1)

qplot(data = e,
      facets = rule.id ~ num.examples,
      x = example.num,
      y = z.len,
      geom = 'line',
      group = worker.id) +
  geom_point(mapping = aes(color = polarity)) +
  scale_color_brewer(palette = "Set1")
```

in the bottom row (zip code), there appears to be a pattern of examples starting off all the same length. but this  is just an artifact of the rule itself being a constraint on length and people tending to give strings of positive examples first.

there don't appear to be very obvious length patterns here.

but there are some polarity templates:
- all positive, then all negative
- shit sandwich: positive, negative, positive
- interleaving


### polarity

```{r}
e = responses %>%
        group_by(example.num) %>%
        summarise(frac.pos = sum(polarity == 'positive') / n(),
                  ci.l = generic.ci(polarity == 'positive')['ci.l'],
                  ci.u = generic.ci(polarity == 'positive')['ci.u'])

qplot(data = e,
      x = example.num,
      y = frac.pos,
      ymin = ci.l,
      ymax = ci.u,
      geom = c('pointrange','line'))
```

interesting -- first example tends to be positive

#### within a cluster, does the first example tend to be positive?

```{r}
e = responses.clustered %>%
  group_by(worker.id, rule.id, cluster.label) %>%
  summarise(first.cluster.example.polarity = polarity[1]) %>%
  ungroup() %>%
  select(first.cluster.example.polarity) %>%
  table

print(e)

prop.test(x = e['positive'], n = sum(e))
```

no. looks like it's just the very first example that tends to be positive

# misc

did 90210 show up as a zip code much?
```{r}
responses %>% filter(rule.id == 'zip-code', string == '90210')
```

no -- only once. i wonder if people were more likely to use their own zip code (could try reconciling with geolocation)


# looking at raw examples

## 3a

## consonants-only

4 people gave "aeiou":
```{r}
filter(responses, rule.id == 'consonants-only') %>% filter(string == 'aeiou') %>% nrow
```


i think this is a good example sequence:

```{r}
filter(responses, worker.id == '1ec52d4', rule.id == 'consonants-only') %>% select(example.num, polarity, string)
```

i think this is a bad example sequence, as it doesn't make clear the difference between positive and negative examples (also, there is an error -- #6 is a negative example but it actually doesn't contain any consonants. DOH -- there are other alternatives to the consonants-only rule besides vowels. you can include numbers too.)

```{r}
filter(responses, worker.id == '26e3c2f', rule.id == 'consonants-only') %>% select(example.num, polarity, string)
```

(initially i was a little worried that this person was just mashing random keys but they didn't do that for the other rules, so it seems unlikely)

an interesting sequence i'm curious how good these examples are:
```{r}
filter(responses, worker.id == '4ba0325', rule.id == 'consonants-only') %>% select(example.num, polarity, string)
```


## delimiters

### no one nested the brackets (either positively or negatively)

```{r}
responses %>% filter(rule.id == 'delimiters')
```


## zip-code

### negative examples tend to be purely numeric (though there are a few exceptions)

```{r}
e = responses %>%
  filter(rule.id == 'zip-code', polarity == 'negative') %>%
  {example.matches(.$string, "[0123456789]+")}

print(table(e))

prop.test(x = sum(e == TRUE), n = length(e))
```


# TODO

- do positive examples tend to be dissimilar from other positive examples? (but similar to a subset of negative examples)
  - metaphor: a positive example as a primary tentpole that supports a number of negative example tentpoles
  - another metaphor: a positive example as a nucleus around which negative examples orbit
  - there are clearly cases where you wouldn't expect this to be true (e.g., for 3a rule, you might give aaa, aaaa, aaaaa)

