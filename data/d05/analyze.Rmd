---
title: "d05a analysis"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries}
library(MASS)
library(plyr)
library(tidyverse)
library(lubridate)
library(memoise)
```

```{r utilities}
generic.ci_ <- function(x, n = 5000, seed = 1) {
  set.seed(seed)
  lenx = length(x)
  
  structure(
    quantile(
      replicate(n, mean(x[sample.int(lenx, replace = TRUE)])),
      c(0.025, 0.975)),
    names=c("ci.l","ci.u"))
}

generic.ci <- memoise(generic.ci_)
```

# read in data

```{r}
results.dir = "production-results/"
assignments = read_csv(paste0(results.dir, "assignments.csv")) %>%
  mutate(accept.time = ymd_hms(accept.time),
         submit.time = ymd_hms(submit.time),
         duration = difftime(submit.time, accept.time, units = "mins"))
responses = read_csv(paste0(results.dir, "responses.csv"),
                     col_types = cols(string = col_character()))

## arrange assignments in order of time
assignments = assignments %>% arrange(accept.time)
```


```{r compute-example-correctness}
regexes = c('3a' = 'aaa+',
            '3a-1' = 'aaaaaa+',
            '3a-2' = '[aA]+',
            'zip-code' = '[0123456789]{5}',
            'zip-code-1' = '.{5}',
            'zip-code-2' = '[0123456789]+',
            'suffix-s' = '.*s\\>',
            'suffix-s-1' = '.*s.*',
            'suffix-s-2' = '.*[a-z].*',
            'delimiters' = "\\[.*\\]",
            'delimiters-1' = '\\[.*',
            'delimiters-2' = '.*\\]'
            )

example.matches = function(example, rx) {
  res = regexpr(pattern = rx, text = example)
  # make sure we match and that the *entire* string is what matches, not a substring
  res > 0 & attr(res, "match.length") == nchar(example)
}
# example.correct(example = 'aaa', rx = 'aaa+')
# example.correct(example = 'baaa', rx = 'aaa+')
# example.correct(example = 'aaaa', rx = 'aaa+')

responses = responses %>%
  mutate(rx = regexes[rule.id])

responses_match = apply(responses[,c('string','rx')],
      1,
      function(e) { example.matches(example = e['string'], rx = e['rx']) })

responses = responses %>%
  mutate(match = responses_match,
         correct = !xor(polarity == 'positive', match)) %>%
  select(-rx, -rule.desc) # hide these cause they're verbose

# # testing
# View(responses %>% select(rule.id, polarity, string, correct, match) %>% arrange(rule.id, polarity))
```


# auxiliary

## how long does the task take?

```{r}
qplot(data = assignments,
      x = as.numeric(duration),
      binwidth = 1,
      color = I('white')) + 
  xlab("duration (minutes)")
```


## what did people think was a fair payment?

```{r}
fair.pay = gsub("\\$","",assignments$fair_pay) %>% 
  as.numeric %>% na.omit
fair.pay = fair.pay[fair.pay < 6]
qplot(x = fair.pay,
      binwidth = 0.1,
      color = I('white')
      )
```

## how old are people?

```{r}
qplot(data = assignments,
      x = age,
      binwidth = 5)
```

they all tend to be older

## what gender are they?

```{r}
table(tolower(substr(assignments$gender, start = 1, stop = 1)))
```

## what is their programming / regex experience?

```{r}
assignments %>% select(worker.id, programming.experience) %>% arrange(desc(nchar(programming.experience)))
```
there's maybe 16 people with some sort of programming experience.

```{r}
assignments %>% select(worker.id, regex.experience) %>% arrange(desc(nchar(regex.experience)))
```

### do the people with regex experience give interesting examples?

```{r}
regex.knowers = c("ebda6ed", "d2f7661", "c8cecf0", "a33a11b", "9eabb06", "1dc006e", "ec8b199", "e12e476")
responses %>% filter(worker.id %in% regex.knowers) %>% select(worker.id, rule.id, example.num, polarity, string)
```



TODO: analyze

## any bugs?

```{r}
assignments %>% select(bugs, worker.id) %>% arrange(desc(nchar(bugs)))
```

largely smooth

## how much did they enjoy the task?

```{r}
assignments %>% select(enjoy) %>% arrange(desc(nchar(enjoy)))
```

It was decent

# research

## how many examples do people give?

```{r, fig.width = 11, fig.height = 3}
e.agg = responses %>% group_by(worker.id, rule.id) %>%
  summarise(num.examples = n()) %>%
  group_by(rule.id, num.examples) %>%
  summarise(freq = n())

xmin = 1 #min(e.agg$num.examples)
xmax = max(e.agg$num.examples)

e.agg$num.examples.fct = factor(e.agg$num.examples, levels = as.character(xmin:xmax))

ggplot(data = e.agg) +
  facet_grid(. ~ rule.id) +
  geom_bar(mapping = aes(x = num.examples.fct, y = freq), stat = 'identity') +
  scale_x_discrete(breaks = as.character(xmin:xmax), drop = FALSE, name = 'number of examples')
```

pub figure:

```{r corpora-sizes, fig.width = 11, fig.height = 3, dev="pdf"}
e.agg = responses %>% group_by(worker.id, rule.id) %>%
  summarise(num.examples = n()) %>%
  group_by(rule.id, num.examples) %>%
  summarise(freq = n())

xmin = 1 #min(e.agg$num.examples)
xmax = max(e.agg$num.examples)

rename.delimiters.rule = function(d) {
  d %>% transform(rule.id = ifelse(rule.id == 'delimiters', 'bracketed', rule.id))
}

e.agg$num.examples.fct = factor(e.agg$num.examples, levels = as.character(xmin:xmax))
e.agg = e.agg %>%
  rename.delimiters.rule %>%
  transform(rule.id = paste0("Rule: ", rule.id))

ggplot(data = e.agg) +
  facet_grid(. ~ rule.id) +
  geom_bar(mapping = aes(x = num.examples.fct, y = freq), stat = 'identity') +
  scale_x_discrete(breaks = as.character(xmin:xmax), drop = FALSE, name = 'Number of examples') +
  ylab("Frequency") +
  theme_classic(14)
```

## q: do 2-examples tend to be *balanced*? i.e., one positive and one negative?

```{r}
e.agg = responses %>%
  group_by(worker.id, rule.id) %>%
  mutate(num.examples = length(string)) %>%
  filter(num.examples == 2) %>%
  group_by(worker.id, rule.id) %>%
  summarise(num.pos = sum(polarity == 'positive'),
            num.neg = sum(polarity == 'negative'))

print(table(e.agg$num.pos, e.agg$num.neg))

# proportion test on the frequency of balanced 2-examples
prop.test(x = sum(e.agg$num.pos == 1), nrow(e.agg))
```

yes, though not significantly (though i don't have a whole lot of data)

### for balanced 2-examples, what is the mean edit distance?

```{r}
e.agg = responses %>%
  group_by(worker.id, rule.id) %>%
  mutate(num.examples = length(string)) %>%
  filter(num.examples == 2, sum(polarity == 'positive') == 1) %>%
  summarise(edit.distance = adist(string)[1,2])

mean.balanced.pair.edit.distance = mean(e.agg$edit.distance)

qplot(data = e.agg,
      x = edit.distance,
      color = I('white'),
      binwidth = 1) + scale_x_continuous(breaks = with(e.agg, min(edit.distance):max(edit.distance))) +
  geom_vline(xintercept = mean.balanced.pair.edit.distance, color = I('red'))
```

mean is red line -- 4.41.

are these examples are closer than you'd expect by chance? (permutation test)

```{r}
e.agg = responses %>%
  group_by(worker.id, rule.id) %>%
  mutate(num.examples = length(string)) %>%
  filter(num.examples == 2, sum(polarity == 'positive') == 1)

workers.and.rules = e.agg[,c('worker.id', 'rule.id')]

balanced.example.pair.edit.distance.bootstrap = function(pool) {
  # sample a permuted dataset:
  # for each rule, shuffle all the examples and then pair them off
  # then for each pair, compute edit distance
  
  syn.df = ddply(pool,
        .(rule.id), function(e) {
          pool.pos = e[e$polarity == 'positive',]$string
          pool.neg = e[e$polarity == 'negative',]$string
          
          syn.pos = sample(pool.pos)
          syn.neg = sample(pool.neg)
          
          syn = rbind(syn.pos, syn.neg)
          
          data.frame(distance = apply(X = syn, MARGIN = 2, F = function(pair) { adist(pair)[1,2] }))
        })
  
  mean(syn.df$dist)
}

time = system.time(bootstrap.samples <- replicate(5000, balanced.example.pair.edit.distance.bootstrap(e.agg)))

writeLines(paste0("elapsed seconds: ", round(unname(time['elapsed']), 1)))

writeLines(paste0('95% ci for bootstrap: ', paste0(quantile(bootstrap.samples, c(0.025, 0.975)), collapse = " - ")))

# one-tailed test: how many bootstrap samples have a mean
# number of clusters less than the observed sample?
sum(bootstrap.samples < mean.balanced.pair.edit.distance) / length(bootstrap.samples)
```

yes, this fewer than you'd expect by chance.

## how many positive examples versus negative examples?

```{r, fig.width = 11, fig.height = 3.8}
e.agg = responses %>% group_by(worker.id, rule.id) %>%
  summarise(num.pos = sum(polarity == "positive"),
            num.neg = sum(polarity == "negative")) %>%
  ungroup() %>%
  group_by(num.pos, num.neg, rule.id) %>%
  summarise(freq = n())

e.viz = e.agg %>% rename.delimiters.rule %>%
  transform(rule.id = paste0("Rule: ", rule.id)) %>%
  rename(Frequency = freq)
  
ggplot(data = e.viz) +
  facet_grid(. ~ rule.id) +
  geom_abline(color = I('gray')) + 
  geom_point(mapping = aes(x = num.pos,
                           y = num.neg,
                           size = Frequency)) +
  scale_x_continuous(name = '# positive examples', breaks = c(0, 4, 8), limits = c(0, 8)) +
  scale_y_continuous(name = '# negative examples', breaks = c(0, 4, 8), limits = c(0, 8)) +
  theme_classic(14) +
  theme(legend.position="bottom")
```

things are somewhat balanced -- people tend to give some negative examples.

stats on suffix-s and zip-code:
```{r}
e.agg = responses %>% group_by(worker.id, rule.id) %>%
  summarise(num.pos = sum(polarity == "positive"),
            num.neg = sum(polarity == "negative"))

with(e.agg %>% filter(rule.id == 'suffix-s'), t.test(x = num.pos, y = num.neg, paired = TRUE))
with(e.agg %>% filter(rule.id == 'zip-code'), t.test(x = num.pos, y = num.neg, paired = TRUE))
```


### q: do people give more positive examples than negative?

simple check -- paired t-test between number of positive and number of negative examples for each trial X user.

```{r}
e.agg = responses %>% group_by(worker.id, rule.id) %>%
  summarise(num.pos = sum(polarity == "positive"),
            num.neg = sum(polarity == "negative"))

with(e.agg, t.test(num.pos, num.neg, paired = TRUE))
```

no, doesn't look like it.


## how related are the examples in edit distance?

```{r}
unique.strings = responses$string %>% unique
cached.distances = adist(unique.strings)
rownames(cached.distances) = unique.strings
colnames(cached.distances) = unique.strings
memoised.adist = function(strings) {
  cached.distances[strings,strings]
}
```


```{r}
# nb: cluster label numbers do not correspond to chronological order
cluster.examples = function(strings, distance.threshold = 2, dist.function = memoised.adist) {
  original.order = strings
    
  distance.matrix = adist(strings)
  
  # for each string, figure out which other strings it's similar to
  # (i.e., has edit distance less than the threshold)
  similarities = Map(1:nrow(distance.matrix),
                     f = function(row.num) {
                       r = distance.matrix[row.num,]
                       which(r <= distance.threshold)
                     })
  
  dirty.clusters = list()
  # print(similarities)
  
  overlap = function(a,b) {
    length(intersect(a,b)) > 0
  }
  
  Map(1:length(strings),
      f = function(i) {
        # j is the index of the previously created cluster that can contain this string
        j = Position(x = dirty.clusters,
                     f = function(dcluster) { i %in% dcluster })
        
        if (is.na(j)) {
          dirty.clusters[[length(dirty.clusters) + 1]] <<- similarities[[i]]
        } else {
          dirty.clusters[[j]] <<- union(dirty.clusters[[j]], similarities[[i]])
        }
      })
  
  ## clean up clusters
  clusters = list()
  for(i in 1:length(dirty.clusters)) {
    x = dirty.clusters[[i]]
    overlaps = unique(c(x, unlist(Filter(clusters, f = function(y) { overlap(x,y) }))))
    nonoverlaps = Filter(clusters, f = function(y) { !overlap(x,y) })
    clusters <- c(list(overlaps), nonoverlaps)
    #print(clusters)
    #browser()
  }

  
  # # return a list of clusters (each cluster is a vector containing strings that are clustered together)
  # Map(clusters, f = function(indices) { strings[indices] })

  ### WIP ###
  # return a data frame with two columns: string and cluster number
  # x = do.call(rbind,
  #         Map(clusters,
  #             1:length(clusters),
  #             f = function(indices, cluster.label) {
  #               data.frame(string = strings[indices],
  #                          cluster.label = cluster.label)
  #             }
  #         ))
  
  cluster.labels = sapply(X = 1:length(strings),
         FUN = function(i) {
           Position(x = clusters, f = function(cluster) { i %in% cluster })[[1]]
         })
  
  cluster.labels
  # 
  # data.frame(string = strings, cluster.label = cluster.labels)
  
}

# # testing
#strings = c("aa","bb","bbbb", "bbbbbb") # should be 1 cluster
#strings = c("aa","ab","xxx","xxyy") # should be 2 clusters
strings = c("11394", "95834", "13094", "52349", "1234b", "j1344", "123b4") # should be 2 clusters, 7 strings
#strings = c("12345", "12a34", "72384", "7238l", "sdfgf", "75348", "98765")
#strings = c("01234", "012a4", "62804", "628041", "y6280", "0123", "280", "a280b")
#strings = c('aaa','aa','aaab','baaaab','bbaaabb')
#strings = c("94301", "40510", "33333", "r2349", "asdfa", "3621", "834920")
cluster.examples(strings, dist.function = adist)
```

TODO: adaptive clustering based on string lengths so that i can handle things like abracadabra vs abrAcAdAbra

### mean number of clusters for an example sequence:

```{r}
responses.clustered = responses %>%
  group_by(worker.id, rule.id) %>%
  mutate(cluster.label = cluster.examples(string)) %>%
  select(-assignment.id, -rule.desc) %>%
  ungroup()

e.agg = responses.clustered %>%
  group_by(worker.id, rule.id) %>%
  summarise(num.clusters = max(cluster.label)) %>%
  group_by(worker.id) %>%
  summarise(mean_num.clusters = mean(num.clusters))

mean.num.clusters = mean(e.agg$mean_num.clusters)

summary(e.agg$mean_num.clusters)
```

### do there tend to be more negative examples within a cluster? 

i think people might come up with one example and then demonstrate various ways it can be perturbed to be a non-example

```{r, fig.width = 10, fig.height = 3}
e = responses.clustered %>%
  group_by(rule.id, worker.id, cluster.label) %>%
  summarise(num.pos = sum(polarity == 'positive'), size = length(polarity)) %>% 
  transform(num.neg = size - num.pos, frac.pos = num.pos / size) %>% filter(size > 1)

stats = ddply(e, .(rule.id), function(e) { c('p.value' = t.test(x = e$num.pos, y = e$num.neg, paired = TRUE)$p.value ) })

e.agg = e %>%
  group_by(rule.id, num.pos, num.neg) %>%
  summarise(n = n())

facet_labeller = function(d) {
  res = d %>% merge(stats) %>% transform(label = paste0(rule.id, " (p = ", round(p.value, 2), ")")) %>%
    select(label) %>% rename(rule.id = label)
  res$rule.id = as.character(res$rule.id)
  res
}

qplot(data = e.agg,
      x = num.pos,
      y = num.neg,
      size = n
      ) + geom_abline() +
  facet_grid(. ~ rule.id, labeller = facet_labeller)
```

3a: no
delimiters: no
suffix-s: no (note also that cluster size is smaller)
zip-code: yes

interesting, there are clusters with 0 positive examples. what are those?
```{r}
rc.agg = responses.clustered %>%
  group_by(rule.id, worker.id, cluster.label) %>% 
  summarise(num.pos = sum(polarity == 'positive'), num.neg = sum(polarity == 'negative'), size = length(polarity), 
         corpus = paste(string, c("positive" = "+", "negative" = "-")[polarity], collapse = " | ")
         ) %>%
  ungroup()

rc.agg %>%
  filter(num.pos == 0, num.neg > 1)
```

not that interesting

what about clusters with 0 negative examples?
```{r}
rc.agg %>%
  filter(num.neg == 0, num.pos > 1) %>%
  select(rule.id, worker.id, corpus)
```

not that interesting -- some of these cases would go away with adaptive clustering.

### call an island a cluster of size 1. do islands tend to be positive or negative?
```{r}
rc.agg %>% filter(size == 1) %>% group_by(rule.id) %>% summarise(fraction.pos = mean(num.pos))
```

looks sort of even.


### does the first example in a non-island cluster tend to be positive?
```{r}
responses.clustered %>% merge(rc.agg) %>%
  filter(size > 1) %>%
  group_by(rule.id, worker.id, cluster.label) %>%
  summarise(first.polarity = polarity[1]) %>%
  group_by(rule.id) %>%
  summarise(mean.first.polarity.positive = mean(first.polarity == 'positive'))
```

overall:
```{r}
responses.clustered %>% merge(rc.agg) %>%
  filter(size > 1) %>%
  group_by(rule.id, worker.id, cluster.label) %>%
  summarise(first.polarity = polarity[1]) %>%
  ungroup() %>% {.$first.polarity == "positive"} %>%
  table() %>%
  chisq.test()
```


### do examples within a cluster tend to be chronologically close together?
```{r}
num.contiguous.spans <- function(xs) {
  xs = sort(xs)
  
  last_x = xs[1] - 1
  num_spans = 1
  in_span = TRUE
  for(x in xs) {
    if (x == last_x + 1) {
      in_span = TRUE
    } else {
      num_spans = num_spans + 1
      in_span = FALSE
    }
    last_x = x
  }
  num_spans
}
# num.contiguous.spans(c(1))
# num.contiguous.spans(c(1,3))
# num.contiguous.spans(c(1,3,4,6))
# num.contiguous.spans(c(1,2,3,4,5,6))
# num.contiguous.spans(c(1, 100,101, 109,110,111, 200))
# num.contiguous.spans(c(2,4,6,8,10))
# num.contiguous.spans(c(1,2, 4, 6, 8, 10, 100,101, 109,110,111, 200))

e.agg = responses.clustered %>% merge(rc.agg) %>%
  filter(size > 1) %>%
  group_by(rule.id, worker.id, cluster.label, size) %>%
  summarise(num.spans = num.contiguous.spans(example.num))

qplot(data =  e.agg %>% group_by(size, num.spans) %>% summarise(n = n()),
      x = size,
      y = num.spans,
      size = n
      )

qplot(data = e.agg,
      x = num.spans,
      geom = 'histogram',
      binwidth = 1,
      color = I('white')
      )
```


### comparison to permutation test

sample random participants by sampling from pool of all participants' responses (note that this is sampling *without* replacement, as people wouldn't give the same example twice)
- runtime note: takes around 1136 seconds for 1000 bootstrap samples. it's a little slow because my clustering function is not vectorized

```{r eval=FALSE}
sample.bootstrap.subject = function(worker.id, rule.id) {
  # get examples given by all participants for this rule
  ## written in non-dplyr syntax because i think it might be faster?
  pool = responses[responses$rule.id == rule.id,]
  pool.pos = pool[pool$polarity == 'positive',]$string
  pool.neg = pool[pool$polarity == 'negative',]$string
  
  # get this worker's examples
  this = pool[pool$worker.id == worker.id,]
  
  num.pos = sum(this$polarity == 'positive')
  num.neg = sum(this$polarity == 'negative')
  
  syn.pos = sample(x = pool.pos, size = num.pos, replace = FALSE)
  syn.neg = sample(x = pool.neg, size = num.neg, replace = FALSE)
  
  c(num.clusters = max(cluster.examples(c(syn.pos, syn.neg))))
}

workers.and.rules = responses.clustered[,c('worker.id', 'rule.id')]

clusters.bootstrap = function() {
  num.clusters = apply(workers.and.rules,
                       1,
                       function(e) { 
                         sample.bootstrap.subject(e['worker.id'], e['rule.id'])
                       })
  
  mean(num.clusters)
}

time = system.time(bootstrap.samples <- replicate(1000, clusters.bootstrap()))

writeLines(paste0("elapsed seconds: ", round(unname(time['elapsed']), 1)))

writeLines(paste0('95% ci for bootstrap: ', paste0(quantile(bootstrap.samples, c(0.025, 0.975)), collapse = " - ")))

# one-tailed test: how many bootstrap samples have a mean
# number of clusters less than the observed sample?
sum(bootstrap.samples < mean.num.clusters) / length(bootstrap.samples)
```

bootstrapping takes ~1.1 seconds per sample

TODO: the ways i've done both permutation tests are, i think, reasonable, but there are also reasonable alternatives. talk through my choices with mike, then write down rationale.

## how many mistakes do people make? (e.g., positive examples that don't actually match or negative examples that do match)


by stimulus:

```{r}
responses %>%
  group_by(rule.id) %>%
  summarise(error.rate = sum(!correct) / n())
```

inspecting errors:

```{r}
responses %>% filter(!correct) %>% select(rule.id, worker.id, string, polarity, match, correct) %>% arrange(rule.id)
```

TODO


by person:
```{r}
e = responses %>%
  group_by(worker.id) %>%
  summarise(error.rate = sum(!correct) / n()) %>% 
  arrange(desc(error.rate))

qplot(data = e,
      x = error.rate,
      geom = 'histogram')
```

there's quite a heavy tail of people that make errors. what's up with the outliers?

```{r}
e %>% filter(error.rate > 0.4) %>% merge(responses) %>%
  select(worker.id, rule.id, example.num, polarity, string)
```

one lazy worker and some people that didn't understand the instructions.

## how long are the examples that people give?

by stimulus:

```{r, fig.width = 11, fig.height = 3}
qplot(data = responses %>% filter(nchar(string) < 25),
      facets = . ~ rule.id,
      x = nchar(string),
      binwidth = 1,
      color = I('white'))
```

by person:

```{r}
qplot(data = responses,
      x = worker.id,
      y = nchar(string), alpha = I(0.5)) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## do people give examples in particular orders? e.g., shorter ones first or positive ones first?

### length

un-zscored:
```{r}
e = responses %>%
  transform(len = nchar(string)) %>%
  group_by(example.num) %>%
  summarise(mean.len = mean(len),
            cl.len = generic.ci(len)['ci.l'],
            cu.len = generic.ci(len)['ci.u']) %>%
  ungroup

qplot(data = e,
      x = example.num,
      y = mean.len,
      ymin = cl.len,
      ymax = cu.len,
      geom = c('pointrange','line'))
```

note: confidence intervals for last two points are actually huge (very few unique data values, e.g., 1)

un-z-scored, broken down by rule:

```{r}
e = responses %>%
  transform(len = nchar(string)) %>%
  group_by(example.num, rule.id) %>%
  summarise(mean.len = mean(len),
            cl.len = generic.ci(len)['ci.l'],
            cu.len = generic.ci(len)['ci.u']) %>%
  ungroup

qplot(data = e,
      facets = . ~ rule.id,
      x = example.num,
      y = mean.len,
      ymin = cl.len,
      ymax = cu.len,
      geom = c('pointrange','line'))
```


z-scoring length per subject per rule:
```{r}
z.score <- function(xs) {
  centered = xs - mean(xs)
  if (length(xs) == 1) {
    centered
  } else {
    # NB: deliberately doesn't catch the case where all xs are the same
    # because i filter for this later on
    centered / sd(xs)
  }
}

e = responses %>%
  transform(len = nchar(string)) %>%
  group_by(worker.id, rule.id) %>%
  mutate(z.len = z.score(len)) %>%
  mutate(z.len = ifelse(is.nan(z.len), 0, z.len)) %>%
  group_by(example.num) %>%
  summarise(mean.z.len = mean(z.len),
            cl.len = generic.ci(z.len)['ci.l'],
            cu.len = generic.ci(z.len)['ci.u']) %>%
  ungroup

qplot(data = e,
      x = example.num,
      y = mean.z.len,
      ymin = cl.len,
      ymax = cu.len,
      geom = c('pointrange','line'))
```

doesn't appear to be any sequencing over all the participants; are there particular cases where this happens? (TODO)


break out previous plot by rule.id:

```{r, fig.width = 11, fig.height = 3}
z.score <- function(xs) {
  centered = xs - mean(xs)
  if (length(xs) == 1) {
    centered
  } else {
    # NB: deliberately doesn't catch the case where all xs are the same
    # because i filter for this later on
    centered / sd(xs)
  }
}

e = responses %>%
  transform(len = nchar(string)) %>%
  group_by(worker.id, rule.id) %>%
  mutate(z.len = z.score(len)) %>%
  mutate(z.len = ifelse(is.nan(z.len), 0, z.len)) %>%
  group_by(example.num, rule.id) %>%
  summarise(mean.z.len = mean(z.len),
            cl.len = generic.ci(z.len)['ci.l'],
            cu.len = generic.ci(z.len)['ci.u']) %>%
  ungroup

qplot(data = e,
      facets = . ~ rule.id,
      x = example.num,
      y = mean.z.len,
      ymin = cl.len,
      ymax = cu.len,
      geom = c('pointrange','line'))
```


for each rule, plot each person's length curve individually:

```{r, fig.width = 8, fig.height = 4, dev="svg"}
e = responses %>%
  transform(len = nchar(string)) %>%
  group_by(worker.id, rule.id) %>%
  mutate(z.len = z.score(len), num.examples = max(example.num)) %>%
  mutate(z.len = ifelse(is.nan(z.len), 0, z.len)) %>%
  filter(num.examples > 1)

qplot(data = e,
      facets = rule.id ~ num.examples,
      x = example.num,
      y = z.len,
      geom = 'line',
      group = worker.id) +
  geom_point(mapping = aes(color = polarity)) +
  scale_color_brewer(palette = "Set1")
```




- 2-examples are balanced
- we see few entirely positive sequences
- possible pattern: all positive followed by all negative


### polarity

```{r}
e = responses %>%
        group_by(example.num) %>%
        summarise(frac.pos = sum(polarity == 'positive') / n(),
                  ci.l = generic.ci(polarity == 'positive')['ci.l'],
                  ci.u = generic.ci(polarity == 'positive')['ci.u'])

qplot(data = e,
      x = example.num,
      y = frac.pos,
      ymin = ci.l,
      ymax = ci.u,
      geom = c('pointrange','line'))
```

interesting -- first example tends to be positive.
also interesting: subsequent examples are roughly a coin flip between positive and negative. so maybe this is where the significant 0.45 more positive examples than negative comes from -- just the first one? (no, see above)

very busy graph. just looking at polarity:
```{r}
e = responses %>%
  transform(len = nchar(string)) %>%
  group_by(worker.id, rule.id) %>%
  mutate(z.len = z.score(len), num.examples = max(example.num)) %>%
  mutate(z.len = ifelse(is.nan(z.len), 0, z.len))

e = ddply(e, .(rule.id), function(ee) {
  # add dummy indices that are used to layout example sequences vertically
  ee = ee %>%
    arrange(num.examples) %>%
    transform(sort.order = 1:nrow(ee)) # merging will change the order so keep a way of restoring it
  dummy.df = data.frame(worker.id = unique(ee$worker.id))
  dummy.df$dummy.id = 1:nrow(dummy.df)
  merge(ee, dummy.df) %>% arrange(sort.order)
})

qplot(data = e,
      facets = . ~ rule.id,
      x = example.num,
      y = dummy.id,
      fill = polarity,
      color = I('white'),
      geom = 'tile') +
  scale_fill_brewer(palette = "Set1")
```



#### within a cluster, does the first example tend to be positive?

```{r}
e = responses.clustered %>%
  group_by(worker.id, rule.id, cluster.label) %>%
  summarise(first.cluster.example.polarity = polarity[1]) %>%
  ungroup() %>%
  select(first.cluster.example.polarity) %>%
  table

print(e)

prop.test(x = e['positive'], n = sum(e))
```

no. looks like it's just the very first example that tends to be positive



## are positive strings further apart than you'd predict by chance?

it turns out that, by chance, you'd expect strings to be pretty far apart; for zip-code, random strings tend to be almost maximally far apart:
```{r}
path.length = function(distances, path) {
  n = length(path)
  sapply(X = 1:n, FUN = function(i) {
    j = (i + 1)
    if (j > n) {
      j = 1
    }
    distances[path[i], path[j]]
  })
}

# length of shortest closed walk
scw.length = function(strings) {
  n = length(strings)
  strings.dist = adist(strings)
  lengths = Map(combinat::permn(1:n),
      f = function(path) {
        sum(path.length(strings.dist, path))
      })
  min(unlist(lengths))
}

bootstrap.samples = replicate(10000, scw.length( stringi::stri_rand_strings(n = 3, length = 5, pattern = "[0-9]") ))

print( paste0(quantile(bootstrap.samples, c(0.025, 0.975)), collapse = " - "))

# one-tailed test: how many bootstrap samples have a mean
# number of clusters less than the observed sample?
sum(bootstrap.samples < 15) / length(bootstrap.samples)

#scw.length(c("55034", "12052", "20212"))
#scw.length(c("12345", "123456", "1234567"))

```



## looking at raw examples

### 3a

```{r}
responses %>% filter(rule.id == '3a') %>% select(-time, -match, -assignment.id) %>% arrange(correct) %>%
  transform(string.and.pol = paste0('"', string, '" ', c(positive = '+', negative = '-')[polarity], 
                                    ifelse(correct, "", "!")
                                    )) %>%
  group_by(worker.id) %>%
  summarise(exs = paste0(string.and.pol, collapse = ' ‖ ')) %>%
  arrange(nchar(exs)) %>%
  select(exs, worker.id)
```


### delimiters


```{r}
responses %>% filter(rule.id == 'delimiters') %>% select(-time, -match, -assignment.id) %>% arrange(correct) %>%
  transform(string.and.pol = paste0('"', string, '" ', c(positive = '+', negative = '-')[polarity], 
                                    ifelse(correct, "", "!")
                                    )) %>%
  group_by(worker.id) %>%
  summarise(exs = paste0(string.and.pol, collapse = ' ‖ '))
```


#### did anyone nest the brackets? or show a special case?

```{r}
responses %>% filter(grepl("\\[\\[", string)) %>% select(worker.id, trial.num, example.num, polarity, string) %>% print
responses %>% filter(grepl("\\]\\]", string)) %>% select(worker.id, trial.num, example.num, polarity, string) %>% print
```

no one nested.

```{r}
responses %>% filter(grepl("\\[\\]", string)) %>% select(worker.id, polarity, string)
```

several people showed the empty brackets special case.
also, dff3ecd and 9eabb06 did a good job -- they showed that the brackets must come at the beginning and end, they can't just be balanced.

### zip-code

```{r}
responses %>% filter(rule.id == 'zip-code') %>% select(-time, -match, -assignment.id) %>% arrange(correct) %>%
  transform(string.and.pol = paste0('"', string, '" ', c(positive = '+', negative = '-')[polarity], 
                                    ifelse(correct, "", "!")
                                    )) %>%
  group_by(worker.id) %>%
  summarise(exs = paste0(string.and.pol, collapse = ' ‖ ')) %>%
  arrange(nchar(exs)) %>%
  select(exs, worker.id)
```



### negative examples are somewhat purely numeric

```{r}
e = responses %>%
  filter(rule.id == 'zip-code', polarity == 'negative') %>%
  {example.matches(.$string, "[0123456789]+")}

print(table(e))

prop.test(x = sum(e == TRUE), n = length(e))
```

fraction isn't significantly different from 0.5 but that's not the right baseline value.
also, this special case is kind of subsumed by the more general distance-to-language measure, so i should use that.

### suffix-s

```{r}
responses %>% filter(rule.id == 'suffix-s') %>% select(-time, -match, -assignment.id) %>% arrange(correct) %>%
  transform(string.and.pol = paste0('"', string, '" ', c(positive = '+', negative = '-')[polarity], 
                                    ifelse(correct, "", "!")
                                    )) %>%
  group_by(worker.id) %>%
  summarise(exs = paste0(string.and.pol, collapse = ' ‖ ')) %>%
  select(exs, worker.id)
```


interesting: d2f7661 gave lots of examples but the absence of related negative examples makes it seem less helpful to me:
their examples are: "eagles" + ‖ "pizzas" + ‖ "friends" + ‖ "asdfssss" + ‖ "3r280us" + ‖ "333333s" + ‖ "(*&(*^%%SDs" + ‖ "fasdfasdf" - ‖ "3333333" - ‖ "s" + ‖ "d" - ‖ "gwegw" - ‖ "eeeeee" -

by contrast, 032d129's examples are fewer but seem more helpful: "cats" + ‖ "dogs" + ‖ "dog" - ‖ "cat" -

# exporting example sequences for learners

for each rule, how many example sequences are correct?

```{r}
valid.responses = responses %>% group_by(worker.id, rule.id) %>% summarise(all.correct = all(correct)) %>% filter(all.correct) %>%
   merge(responses) %>% select(-assignment.id, -time, -match, -correct, -all.correct) %>%
  arrange(worker.id, rule.id, example.num)
```

```{r}
valid.responses %>% group_by(rule.id) %>% summarise(num.usable.sequences = length(unique(worker.id)))
```


around 30; plenty

## look through examples to make sure they aren't weird

```{r}
pretty.print.examples = function(d) {
  d %>%
    group_by(worker.id) %>% 
    mutate(num.examples = length(string),
           string.and.pol = paste0('"', string, '" ', c(positive = '+', negative = '-')[polarity])) %>%
    group_by(worker.id, num.examples) %>%
    summarise(exs = paste0(string.and.pol, collapse = ' ‖ ')) %>%
    # sort by number of examples
    arrange(num.examples) %>% select(-num.examples) %>%
    select(exs, worker.id)
}
```


```{r}
valid.responses %>%  filter(rule.id == '3a') %>% pretty.print.examples
```

```{r}
valid.responses %>%  filter(rule.id == 'zip-code') %>% pretty.print.examples
```


```{r}
valid.responses %>%  filter(rule.id == 'suffix-s') %>% pretty.print.examples
```

get rid of fe23a85 (thought they had to separate each character by commas)

```{r}
valid.responses = filter(valid.responses, !(rule.id == 'suffix-s' & worker.id == 'fe23a85'))
```


```{r}
valid.responses %>%  filter(rule.id == 'delimiters') %>% pretty.print.examples
```

get rid of:
- 3808bfe: embedded multiple examples within a single text entry box (a real shame, too)
- ad928f0: embedded linguistic instruction in examples
- fe23a85: again with the commas

```{r}
valid.responses = valid.responses %>% filter(!(rule.id == 'delimiters' & worker.id %in% c('3808bfe','ad928f0','fe23a85')))
```



now, after exclusions, how many sequences do we have for each rule?

```{r}
valid.responses %>% group_by(rule.id) %>% summarise(num.usable.sequences = length(unique(worker.id)))
```

```{r}
valid.responses.shuffled = valid.responses %>% select(worker.id, rule.id) %>% distinct() %>% sample_frac() %>%
  transform(order = 1:length(worker.id)) %>%
  merge(valid.responses) %>%
  arrange(order, example.num) %>%
  rename(teacher.id = worker.id)

write(x = paste0("module.exports = ", toJSON(valid.responses.shuffled, pretty = FALSE)), file = "curricula.js")
```













```{r}
pretty.print.examples2 = function(d) {
  d %>%
    group_by(worker.id, rule.id) %>% 
    mutate(num.examples = length(string),
           string.and.pol = paste0('"', string, '" ', c(positive = '+', negative = '-')[polarity])) %>%
    group_by(worker.id, num.examples, rule.id) %>%
    summarise(exs = paste0(string.and.pol, collapse = ' ‖ ')) %>%
    # sort by number of examples
    arrange(num.examples) %>% select(-num.examples) %>%
    select(exs, worker.id, rule.id)
}
```


experimenting with dimensionality reduction visualization

```{r}
e = responses %>% filter(worker.id == 'dff3ecd', rule.id == 'zip-code')  %>% select(string, polarity) %>%
  transform(order = 1:length(string))

dimred = cmdscale(dist.strings) %>% transform(string = strings) %>% merge(e) %>% arrange(order) %>%
  transform(string = paste0("(", order, ") ", string)) %>%
  rename(x = X1, y = X2)

dimred$dx = c(0,   0.4, 0,   0,  -0.5, 0)
dimred$dy = c(0.3, 0,   0.3, 0.3, 0,   0.3)


# ggplot(data = dimred) +
#   geom_point(mapping = aes(x = x, y = y, shape = polarity, color = polarity), size = 5) +
#   geom_text(mapping = aes(x = x + dx, y = y + dy, label = string, color = polarity)) + 
#   scale_shape_manual(values = c("positive" = 1, "negative" = 4)) + 
#   scale_color_manual(values = c("negative" = "firebrick2", "positive" =  "darkseagreen")) +
#   geom_path(mapping = aes(x = x, y = y), color = 'cornflowerblue') + theme_classic()


p = ggplot(data = dimred) +
  geom_point(mapping = aes(x = x, y = y, shape = polarity, color = string), size = 5) +
  scale_shape_manual(values = c("positive" = 1, "negative" = 4)) +
  #scale_color_manual(values = c("negative" = "firebrick2", "positive" =  "darkseagreen")) +
  geom_path(mapping = aes(x = x, y = y), color = 'cornflowerblue') + theme_classic()

#direct.label(p)
```



# looking at distractors

```{r}
valid.responses %>%  filter(rule.id %in% c('3a-1','3a-2')) %>% pretty.print.examples
```


the "must have at least 6 letters" thing is annoying -- get rid of 4719db6

```{r}
valid.responses %>%  filter(rule.id %in% c('suffix-s-1','suffix-s-2')) %>% pretty.print.examples
```

again 4719db6 is problematic

```{r}
valid.responses %>%  filter(rule.id %in% c('zip-code-1','zip-code-2')) %>% pretty.print.examples
```

```{r}
valid.responses %>%  filter(rule.id %in% c('delimiters-1','delimiters-2')) %>% pretty.print.examples
```

export these distractor corpora:


```{r}
distractor.corpora.j = valid.responses %>%  filter(rule.id %in% c('zip-code-1','zip-code-2','3a-1','3a-2','delimiters-1','delimiters-2','suffix-s-1','suffix-s-2')) %>%
  rename(teacher.id = worker.id) %>%
  toJSON()


```


```{r}
texify.examples = function(polarity, string) {
  pols = ifelse(polarity == 'positive', "+", "-")
  strings = paste0("{\\color{magenta} {\\tt ", string, "}}")
  paste0("\\item ", paste0(strings, " ", pols, " ", collapse = ", "))
}

writeLines(yy %>% group_by(rule.id, worker.id) %>% summarise(tex = texify.examples(polarity, string)) %>% ungroup() %>% {.$tex})
```

